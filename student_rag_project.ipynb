{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student RAG Project - Guided Implementation\n",
    "\n",
    "## Welcome!\n",
    "\n",
    "In this project, you'll build a **RAG (Retrieval-Augmented Generation)** system that can answer questions about your documents.\n",
    "\n",
    "### What You'll Learn:\n",
    "- âœ… File I/O (reading documents)\n",
    "- âœ… String manipulation (text chunking)\n",
    "- âœ… Functions and parameters\n",
    "- âœ… Lists and dictionaries\n",
    "- âœ… Loops and conditionals\n",
    "- âœ… Basic calculations and statistics\n",
    "\n",
    "### What's Provided for You:\n",
    "- âœ… Embedding model (converts text to numbers)\n",
    "- âœ… Vector database (stores and searches embeddings)\n",
    "- âœ… LLM connection (generates answers)\n",
    "\n",
    "### Your Tasks:\n",
    "You'll complete **TODO sections** marked with `# TODO:` comments.\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking setup...\n",
      "âœ“ chromadb is installed\n",
      "âœ“ sentence_transformers is installed\n",
      "âœ“ requests is installed\n",
      "\n",
      "âœ“ All required packages are installed!\n",
      "You're ready to start!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the pre-built helper module\n",
    "from rag_helpers import (\n",
    "    EmbeddingModel,\n",
    "    VectorDatabase,\n",
    "    LLM,\n",
    "    Timer,\n",
    "    print_separator,\n",
    "    print_search_results,\n",
    "    print_rag_answer,\n",
    "    check_setup\n",
    ")\n",
    "\n",
    "# Import standard Python libraries you'll use\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "# Check if everything is installed correctly\n",
    "check_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration\n",
    "\n",
    "Set up the basic settings for your RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Documents folder: my_docs\n",
      "  Chunk size: 1000 characters\n",
      "  Overlap: 150 characters\n",
      "  Top-K results: 3\n",
      "\n",
      "Checking files in documents folder:\n",
      " - Document 1 - Match Duration and Basic Structure.txt\n",
      " - Document 10 - Restarts, Drop Balls, and Advantage Rule.txt\n",
      " - Document 2 - Offside Rule.txt\n",
      " - Document 3 - Fouls and Misconduct.txt\n",
      " - Document 4 - Free Kicks and Penalty Kicks.txt\n",
      " - Document 5 - Throw-Ins, Goal Kicks, and Corner Kicks.txt\n",
      " - Document 6 - Yellow and Red Cards.txt\n",
      " - Document 7 - Goalkeeper Rules and Handling Restrictions.txt\n",
      " - Document 8 - Substitutions and Player Roles.txt\n",
      " - Document 9 - Field Layout and Equipment Requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change this to point to YOUR documents folder\n",
    "DOCS_FOLDER = Path(\"./my_docs\")\n",
    "\n",
    "# Chunking settings (you can experiment with these!)\n",
    "CHUNK_SIZE = 1000      # How many characters per chunk\n",
    "OVERLAP = 150          # How many characters overlap between chunks\n",
    "\n",
    "# How many results to retrieve for each query\n",
    "TOP_K = 3\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Documents folder: {DOCS_FOLDER}\")\n",
    "print(f\"  Chunk size: {CHUNK_SIZE} characters\")\n",
    "print(f\"  Overlap: {OVERLAP} characters\")\n",
    "print(f\"  Top-K results: {TOP_K}\")\n",
    "\n",
    "# sanity check to ensure our files are detected\n",
    "print(\"\\nChecking files in documents folder:\")\n",
    "for file in DOCS_FOLDER.iterdir():\n",
    "    print(\" -\", file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #1: Document Loading\n",
    "\n",
    "**Your Task:** Write a function to load all text files from a folder.\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through all `.txt` files in the folder\n",
    "2. Read each file's content\n",
    "3. Store the content and filename in a dictionary\n",
    "4. Return a list of these dictionaries\n",
    "\n",
    "**Python concepts:** File I/O, loops, dictionaries, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 10 documents\n",
      "\n",
      "First document: Document 1 - Match Duration and Basic Structure.txt\n",
      "Content preview: Match Duration and Basic Structure\n",
      "Soccer is played in two halves, and each half lasts 45 minutes in most standard competitions. Together, the full match is 90 minutes of regular time. After the first...\n"
     ]
    }
   ],
   "source": [
    "def load_documents(folder_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load all text documents from a folder.\n",
    "\n",
    "    Args:\n",
    "        folder_path: Path to folder containing .txt files\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries, each containing:\n",
    "        - 'content': the text content of the file\n",
    "        - 'filename': the name of the file\n",
    "\n",
    "    Example:\n",
    "        [\n",
    "            {'content': 'This is doc 1...', 'filename': 'doc1.txt'},\n",
    "            {'content': 'This is doc 2...', 'filename': 'doc2.txt'}\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create an empty list to store documents\n",
    "    # 2. Use Path(folder_path).glob(\"*.txt\") to find all .txt files\n",
    "    # 3. For each file:\n",
    "    #    - Open it with open(file_path, 'r', encoding='utf-8')\n",
    "    #    - Read the content with .read()\n",
    "    #    - Create a dictionary with 'content' and 'filename'\n",
    "    #    - Append to your list\n",
    "    # 4. Return the list\n",
    "\n",
    "    documents = []  # Start with empty list\n",
    "\n",
    "    # Your code here:\n",
    "    folder = Path(folder_path)\n",
    "\n",
    "    for file_path in folder.glob(\"*.txt\"):\n",
    "        # Open and read the file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Create a dictionary\n",
    "        doc_dict = {\n",
    "            \"content\": content,\n",
    "            \"filename\": file_path.name\n",
    "        }\n",
    "\n",
    "        # Add to documents list\n",
    "        documents.append(doc_dict)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Test your function\n",
    "documents = load_documents(DOCS_FOLDER)\n",
    "\n",
    "# Display first document (if any were loaded)\n",
    "if documents:\n",
    "    print(f\"\\nFirst document: {documents[0]['filename']}\")\n",
    "    print(f\"Content preview: {documents[0]['content'][:200]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸  No documents loaded! Check your folder path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #2: Text Chunking Function\n",
    "\n",
    "**Your Task:** Write a function to split long text into smaller chunks with overlap.\n",
    "\n",
    "**Why?** Long documents are too big for embeddings. We need to split them into smaller pieces.\n",
    "\n",
    "**What to do:**\n",
    "1. Start at the beginning of the text\n",
    "2. Take a chunk of `chunk_size` characters\n",
    "3. Move forward by `chunk_size - overlap` characters\n",
    "4. Repeat until you reach the end\n",
    "\n",
    "**Python concepts:** String slicing, loops, lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text length: 800 characters\n",
      "Number of chunks: 10\n",
      "\n",
      "First chunk: This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This\n",
      "Second chunk: This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The text to split\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: How many characters to overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "\n",
    "    Example:\n",
    "        text = \"This is a long document...\"\n",
    "        chunks = chunk_text(text, chunk_size=100, overlap=20)\n",
    "        # Returns: ['This is a long...', 'long document...']\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create an empty list to store chunks\n",
    "    # 2. Start with position = 0\n",
    "    # 3. While position < len(text):\n",
    "    #    - Extract chunk from position to position+chunk_size\n",
    "    #    - Add chunk to list (if not empty)\n",
    "    #    - Move position forward by (chunk_size - overlap)\n",
    "    # 4. Return the list of chunks\n",
    "\n",
    "    chunks = []  # Start with empty list\n",
    "    position = 0  # Start at beginning\n",
    "\n",
    "    # Your code here:\n",
    "    while position < len(text):\n",
    "        # Extract a chunk\n",
    "        chunk = text[position : position + chunk_size]\n",
    "\n",
    "        # Add to chunks list\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # Move position forward\n",
    "        position += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Test your chunking function\n",
    "test_text = \"This is a test. \" * 50  # Create a long test string\n",
    "test_chunks = chunk_text(test_text, chunk_size=100, overlap=20)\n",
    "\n",
    "print(f\"Test text length: {len(test_text)} characters\")\n",
    "print(f\"Number of chunks: {len(test_chunks)}\")\n",
    "print(f\"\\nFirst chunk: {test_chunks[0]}\")\n",
    "if len(test_chunks) > 1:\n",
    "    print(f\"Second chunk: {test_chunks[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #3: Process All Documents into Chunks\n",
    "\n",
    "**Your Task:** Use your chunking function to split ALL documents into chunks and create metadata.\n",
    "\n",
    "**What to do:**\n",
    "1. Loop through each document\n",
    "2. Chunk the document's content\n",
    "3. For each chunk, create metadata (which file it came from, which chunk number)\n",
    "4. Store everything in a list\n",
    "\n",
    "**Python concepts:** Nested loops, dictionaries, enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 20 chunks from 10 documents\n",
      "\n",
      "Example chunk:\n",
      "  Source: Document 1 - Match Duration and Basic Structure.txt\n",
      "  Chunk ID: 0\n",
      "  Text: Match Duration and Basic Structure\n",
      "Soccer is played in two halves, and each half lasts 45 minutes in most standard competitions. Together, the full match is 90 minutes of regular time. After the first...\n"
     ]
    }
   ],
   "source": [
    "def process_documents(documents: List[Dict[str, str]],\n",
    "                     chunk_size: int = 500,\n",
    "                     overlap: int = 50) -> tuple:\n",
    "    \"\"\"\n",
    "    Process all documents into chunks with metadata.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dictionaries\n",
    "        chunk_size: Size of each chunk\n",
    "        overlap: Overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (chunk_texts, chunk_metadatas)\n",
    "        - chunk_texts: List of chunk strings\n",
    "        - chunk_metadatas: List of metadata dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create two empty lists: chunk_texts and chunk_metadatas\n",
    "    # 2. For each document:\n",
    "    #    - Get the document's content and filename\n",
    "    #    - Use your chunk_text() function to split it\n",
    "    #    - For each chunk (use enumerate to get index):\n",
    "    #      - Add chunk text to chunk_texts\n",
    "    #      - Create metadata dict with 'source' and 'chunk_id'\n",
    "    #      - Add metadata to chunk_metadatas\n",
    "    # 3. Return both lists as a tuple\n",
    "\n",
    "    chunk_texts = []\n",
    "    chunk_metadatas = []\n",
    "\n",
    "    # Your code here:\n",
    "    for doc in documents:\n",
    "        # Get document content and filename\n",
    "        content = doc[\"content\"]\n",
    "        filename = doc[\"filename\"]\n",
    "\n",
    "        # Chunk the document\n",
    "        chunks = chunk_text(content, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "        # For each chunk:\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            #   - Add chunk text to chunk_texts\n",
    "            chunk_texts.append(chunk)\n",
    "            #   - Create metadata dictionary\n",
    "            metadata = {\n",
    "                \"source\": filename,\n",
    "                \"chunk_id\": idx\n",
    "            }\n",
    "\n",
    "            #   - Add metadata to chunk_metadatas\n",
    "            chunk_metadatas.append(metadata)\n",
    "\n",
    "\n",
    "    print(f\"âœ“ Created {len(chunk_texts)} chunks from {len(documents)} documents\")\n",
    "    return chunk_texts, chunk_metadatas\n",
    "\n",
    "\n",
    "# Process all documents\n",
    "chunk_texts, chunk_metadatas = process_documents(documents, CHUNK_SIZE, OVERLAP)\n",
    "\n",
    "# Display example\n",
    "if chunk_texts:\n",
    "    print(f\"\\nExample chunk:\")\n",
    "    print(f\"  Source: {chunk_metadatas[0]['source']}\")\n",
    "    print(f\"  Chunk ID: {chunk_metadatas[0]['chunk_id']}\")\n",
    "    print(f\"  Text: {chunk_texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Built: Create Embeddings and Store in Database\n",
    "\n",
    "This part uses the pre-built helpers. Just run these cells - no coding needed! âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model...\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "âœ“ Model loaded!\n",
      "\n",
      "Creating embeddings...\n",
      "Embedding 20 texts...\n",
      "âœ“ Complete!\n",
      "âœ“ Created 20 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model (pre-built)\n",
    "print(\"Initializing embedding model...\")\n",
    "embedder = EmbeddingModel()\n",
    "\n",
    "# Create embeddings for all chunks (pre-built)\n",
    "print(\"\\nCreating embeddings...\")\n",
    "embeddings = embedder.embed_multiple(chunk_texts)\n",
    "print(f\"âœ“ Created {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vector database...\n",
      "âœ“ Vector database initialized\n",
      "  Collection: student_rag\n",
      "  Current documents: 256\n",
      "\n",
      "Adding chunks to database...\n",
      "âœ“ Added 20 chunks to database\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector database (pre-built)\n",
    "print(\"Initializing vector database...\")\n",
    "vector_db = VectorDatabase()\n",
    "\n",
    "# Add chunks to database (pre-built)\n",
    "print(\"\\nAdding chunks to database...\")\n",
    "vector_db.add_chunks(chunk_texts, embeddings, chunk_metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Ollama LLM...\n",
      "âœ“ LLM initialized: mistral at http://127.0.0.1:11434\n",
      "\n",
      "Testing LLM connection...\n",
      "âœ“ LLM is working!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM connection (pre-built)\n",
    "print(\"Connecting to Ollama LLM...\")\n",
    "llm = LLM()\n",
    "\n",
    "\n",
    "# Test the connection\n",
    "print(\"\\nTesting LLM connection...\")\n",
    "if llm.test_connection():\n",
    "    print(\"âœ“ LLM is working!\")\n",
    "else:\n",
    "    print(\"âš ï¸  LLM connection failed! Make sure Docker container is running.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #4: RAG Query Function\n",
    "\n",
    "**Your Task:** Write the main RAG function that ties everything together!\n",
    "\n",
    "**What to do:**\n",
    "1. Embed the user's question\n",
    "2. Search the database for similar chunks\n",
    "3. Build a prompt with the retrieved context\n",
    "4. Ask the LLM to answer based on the context\n",
    "5. Return the answer and metadata\n",
    "\n",
    "**Python concepts:** Functions, string formatting, dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG query function defined!\n"
     ]
    }
   ],
   "source": [
    "def rag_query(question: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "    Args:\n",
    "        question: The user's question\n",
    "        top_k: How many chunks to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'question': the original question\n",
    "        - 'answer': the LLM's answer\n",
    "        - 'sources': list of source filenames\n",
    "        - 'contexts': list of retrieved chunks\n",
    "        - 'time': how long it took\n",
    "    \"\"\"\n",
    "\n",
    "    # Start timer\n",
    "    timer = Timer()\n",
    "    timer.start()\n",
    "\n",
    "    # TODO: Implement the RAG pipeline!\n",
    "    # HINTS:\n",
    "    # 1. Embed the question using: embedder.embed_text(question)\n",
    "    # 2. Search database using: vector_db.search(query_embedding, top_k)\n",
    "    # 3. Extract retrieved chunks and metadata from search results:\n",
    "    #    - retrieved_chunks = results['documents'][0]\n",
    "    #    - retrieved_metadata = results['metadatas'][0]\n",
    "    # 4. Build context by joining chunks with newlines\n",
    "    # 5. Create prompt (template below)\n",
    "    # 6. Generate answer using: llm.generate_answer(prompt)\n",
    "    # 7. Extract source filenames from metadata\n",
    "    # 8. Return everything in a dictionary\n",
    "\n",
    "    # Step 1: Embed question\n",
    "    query_embedding = embedder.embed_text(question)\n",
    "\n",
    "    # Step 2: Search database\n",
    "    results = vector_db.search(query_embedding, top_k)\n",
    "\n",
    "    # Step 3: Extract results\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "    retrieved_metadata = results[\"metadatas\"][0]\n",
    "\n",
    "    # Step 4: Build context\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # Step 5: Create prompt (use this template)\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 6: Generate answer\n",
    "    answer = llm.generate_answer(prompt)\n",
    "\n",
    "    # Step 7: Extract sources\n",
    "    sources = [meta[\"source\"] for meta in retrieved_metadata]\n",
    "\n",
    "    # Stop timer\n",
    "    elapsed_time = timer.stop()\n",
    "\n",
    "    # Step 8: Return results\n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': sources,\n",
    "        'contexts': retrieved_chunks,\n",
    "        'time': elapsed_time\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ“ RAG query function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Your RAG System!\n",
    "\n",
    "Let's try asking some questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== RAG ANSWER ========================\n",
      "\n",
      "QUESTION:  Please tell me who is the best player? Ronaldo or Messi?\n",
      "\n",
      "ANSWER:\n",
      " It's subjective to determine who the \"best\" player between Cristiano Ronaldo and Lionel Messi is, as their performances and achievements vary over time. Both have won multiple individual awards and consistently perform at a high level in international football and for their respective clubs. The choice often depends on personal preferences based on factors such as style of play, team success, or specific accomplishments.\n",
      "\n",
      "SOURCES: Document 8 - Substitutions and Player Roles.txt\n",
      "TIME: 8.94 seconds\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test question 1\n",
    "result = rag_query(\" Please tell me who is the best player? Ronaldo or Messi?\")\n",
    "\n",
    "# Pretty print the result\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question!\n",
    "my_question = \"What conditions must be met for a player to be offside?\"  \n",
    "\n",
    "result = rag_query(my_question)\n",
    "print_rag_answer(\n",
    "    result['question'],\n",
    "    result['answer'],\n",
    "    result['sources'],\n",
    "    result['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #5: Create Test Dataset\n",
    "\n",
    "**Your Task:** Create a list of test questions to evaluate your RAG system.\n",
    "\n",
    "**What to do:**\n",
    "1. Think of 10 questions your documents can answer\n",
    "2. For each question, write the expected answer\n",
    "3. Store them in a structured format\n",
    "\n",
    "**Python concepts:** Lists, dictionaries, data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your test dataset!\n",
    "# HINTS:\n",
    "# Create a list of dictionaries\n",
    "# Each dictionary should have:\n",
    "#   - 'question': the test question\n",
    "#   - 'expected_answer': what you expect the answer to include\n",
    "#   - 'category': type of question (factual, inferential, etc.)\n",
    "\n",
    "test_questions = [\n",
    "    # Example (replace with your own!):\n",
    "    # ---------- FACTUAL (4) ----------\n",
    "    {\n",
    "        'question': 'How long is a standard soccer match and what is the purpose of stoppage time?',\n",
    "        'expected_answer': 'A standard soccer match lasts 90 minutes, divided into two 45-minute halves. Stoppage time compensates for interruptions such as injuries, substitutions, and delays.',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What conditions must be met for a player to be offside?',\n",
    "        'expected_answer': 'A player is offside if they are ahead of both the ball and the second-last defender in the opponentsâ€™ half at the moment the ball is played to them.',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Do only players receive yellow or red cards?',\n",
    "        'expected_answer': 'No, in addition to players, team officials such as coaches or staff can also receive yellow or red cards for inappropriate behavior during a match.',\n",
    "        'category': 'factual'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How many substitutions are typically allowed in a match?',\n",
    "        'expected_answer': 'Most competitions allow three to five substitutions depending on the league or tournament rules.',\n",
    "        'category': 'factual_basic'\n",
    "    },\n",
    "\n",
    "    # ---------- INFERENTIAL (3) ----------\n",
    "    {\n",
    "        'question': 'Why is receiving a red card early in the match a major disadvantage for a team?',\n",
    "        'expected_answer': 'A red card forces a team to continue with one fewer player, reducing defensive stability and limiting attacking opportunities for the rest of the match.',\n",
    "        'category': 'inferential'\n",
    "    },\n",
    "    {\n",
    "        'question': 'If a defender commits a careless foul outside the box, what restart is awarded and why?',\n",
    "        'expected_answer': 'A direct free kick is awarded because the foul occurred outside the penalty area and was judged careless, giving the attacking team a chance to restart play.',\n",
    "        'category': 'inferential'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Why might a team choose to commit a tactical foul?',\n",
    "        'expected_answer': 'Teams commit tactical fouls to stop counterattacks, break the opponentâ€™s momentum, or regain defensive shape when vulnerable.',\n",
    "        'category': 'inferential'\n",
    "    },\n",
    "\n",
    "    # ---------- DEEP_KNOWLEDGE (2) ----------\n",
    "    {\n",
    "        'question': 'How does a referee keep control of the game?',\n",
    "        'expected_answer': 'Referees maintain control by enforcing the rules, issuing cautions or cards when needed, using clear communication, and managing player behavior.',\n",
    "        'category': 'deep_knowledge'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What usually happens if a team keeps committing minor fouls?',\n",
    "        'expected_answer': 'Repeated minor fouls often lead to warnings, yellow cards, or loss of match control for the offending team.',\n",
    "        'category': 'deep_knowledge'\n",
    "    },\n",
    "\n",
    "    # ---------- FUNNY (3) ----------\n",
    "    {\n",
    "        'question': 'What is Cristiano Ronaldoâ€™s favorite food?',\n",
    "        'expected_answer': 'The documents do not contain information about Cristiano Ronaldoâ€™s favorite food.',\n",
    "        'category': 'funny'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Can a goalkeeper drink tea during the match while waiting for the ball?',\n",
    "        'expected_answer': 'The documents do not mention tea drinking, but goalkeepers must stay alert and ready, so it would not be practical.',\n",
    "        'category': 'funny'\n",
    "    },\n",
    "    {\n",
    "        'question': 'If a player celebrates by breakdancing on the field, is it allowed?',\n",
    "        'expected_answer': 'The documents do not specifically mention breakdancing, but unsafe or excessive celebrations may lead to a caution.',\n",
    "        'category': 'funny'\n",
    "    },\n",
    "\n",
    "    # ---------- EDGE_CASES (5) ----------\n",
    "    {\n",
    "        'question': 'What happens if both teams commit fouls at the same time?',\n",
    "        'expected_answer': 'The referee determines which foul occurred first or which offense had greater impact, then restarts play accordingly.',\n",
    "        'category': 'edge_cases'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What happens if a player wears a formal suit during the match?',\n",
    "        'expected_answer': 'Formal clothing does not meet equipment requirements, so the player would be asked to change before participating.',\n",
    "        'category': 'edge_cases'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What happens if one of the referees gets injured?',\n",
    "        'expected_answer': 'A substitute referee or another official typically replaces the injured referee depending on match regulations.',\n",
    "        'category': 'edge_cases'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Is it legal to score directly from a throw-in?',\n",
    "        'expected_answer': 'No, a goal cannot be scored directly from a throw-in according to the rules.',\n",
    "        'category': 'edge_cases'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Can the goalkeeper receive a red card just for touching the ball?',\n",
    "        'expected_answer': 'No, touching the ball alone is not a red card offense; a red card requires a serious violation such as violent conduct.',\n",
    "        'category': 'edge_cases'\n",
    "    },\n",
    "\n",
    "    # ---------- Tactical/ Strategy (3) ----------\n",
    "    {\n",
    "        'question': 'Why might a team target the wings instead of the center?',\n",
    "        'expected_answer': 'The wings often provide more attacking space, fewer defenders, and better opportunities for crossing into the box.',\n",
    "        'category': 'tactical_strategy'\n",
    "    },\n",
    "    {\n",
    "        'question': 'How does a high defensive line affect offside calls?',\n",
    "        'expected_answer': 'A high defensive line increases the likelihood of attackers being caught offside because defenders play closer to midfield.',\n",
    "        'category': 'tactical_strategy'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Why might a team commit intentional fouls in midfield?',\n",
    "        'expected_answer': 'Intentional midfield fouls slow down attacks, disrupt the opponentâ€™s rhythm, and allow the defense to reset.',\n",
    "        'category': 'tactical_strategy'\n",
    "    },\n",
    "\n",
    "    # ---------- OUT_OF_DOMAIN (3) ----------\n",
    "    {\n",
    "        'question': 'How many galaxies exist in the observable universe?',\n",
    "        'expected_answer': 'The documents only cover soccer rules, so they do not contain information about galaxies or astronomy.',\n",
    "        'category': 'out_of_domain'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Who invented the first electric piano?',\n",
    "        'expected_answer': 'The documents include soccer rule information only and do not cover musical instrument history.',\n",
    "        'category': 'out_of_domain'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the boiling point of liquid nitrogen?',\n",
    "        'expected_answer': 'The documents do not discuss scientific measurements such as the boiling point of liquid nitrogen.',\n",
    "        'category': 'out_of_domain'\n",
    "    },\n",
    "\n",
    "    # ---------- HYPOTHETICAL_SCENARIOS (2) ----------\n",
    "    {\n",
    "        'question': 'How would the offside rule apply if soccer was played with flying drones?',\n",
    "        'expected_answer': 'The offside rule is designed for human players; drone soccer would require new rules based on drone movement and positioning.',\n",
    "        'category': 'hypothetical_scenarios'\n",
    "    },\n",
    "    {\n",
    "        'question': 'If soccer had three goals instead of two, how would restarts change?',\n",
    "        'expected_answer': 'The documents do not describe three-goal soccer, but restart rules would likely need adjustments to accommodate a different field layout.',\n",
    "        'category': 'hypothetical_scenarios'\n",
    "    }   \n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(test_questions)} test questions\")\n",
    "print(f\"\\nExample question:\")\n",
    "print(f\"  Q: {test_questions[0]['question']}\")\n",
    "print(f\"  Expected: {test_questions[0]['expected_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #6: Calculate Evaluation Metrics\n",
    "\n",
    "**Your Task:** Write functions to measure how well your RAG system performs.\n",
    "\n",
    "**Python concepts:** Functions, calculations, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_latency(results: List[Dict]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate average response time.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries (each has 'time' field)\n",
    "\n",
    "    Returns:\n",
    "        Average time in seconds\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Extract all 'time' values from results\n",
    "    # 2. Sum them up\n",
    "    # 3. Divide by the number of results\n",
    "    # 4. Return the average\n",
    "\n",
    "    # Your code here:\n",
    "    total_time = sum(result['time'] for result in results)\n",
    "\n",
    "    # Calculate sum and average\n",
    "    avg_time = total_time / len(results) if results else 0.0\n",
    "\n",
    "\n",
    "    return avg_time \n",
    "\n",
    "\n",
    "def count_successful_retrievals(results: List[Dict]) -> int:\n",
    "    \"\"\"\n",
    "    Count how many queries successfully retrieved context.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        Number of successful retrievals\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Start with count = 0\n",
    "    # 2. For each result:\n",
    "    #    - Check if 'contexts' is not empty\n",
    "    #    - If yes, increment count\n",
    "    # 3. Return count\n",
    "\n",
    "    # Your code here:\n",
    "    count = 0\n",
    "    # Count successful retrievals\n",
    "    for result in results:\n",
    "        # Successful if 'contexts' is NOT empty\n",
    "        if result['contexts']:\n",
    "            count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_all_sources(results: List[Dict]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get unique list of all sources used.\n",
    "\n",
    "    Args:\n",
    "        results: List of result dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of unique source filenames\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create an empty set (sets automatically keep unique values)\n",
    "    # 2. For each result:\n",
    "    #    - Get the 'sources' list\n",
    "    #    - Add each source to the set\n",
    "    # 3. Convert set to list and return\n",
    "\n",
    "    # Your code here:\n",
    "    all_sources = set()\n",
    "\n",
    "    # Collect all sources\n",
    "    for result in results:\n",
    "        # result['sources'] is a list â†’ add each\n",
    "        for src in result['sources']:\n",
    "            all_sources.add(src)\n",
    "\n",
    "    return list(all_sources)\n",
    "\n",
    "\n",
    "print(\"âœ“ Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TODO #7: Run Complete Evaluation\n",
    "\n",
    "**Your Task:** Test your RAG system with all test questions and calculate metrics.\n",
    "\n",
    "**Python concepts:** Loops, function calls, data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(test_questions: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run RAG system on all test questions.\n",
    "\n",
    "    Args:\n",
    "        test_questions: List of test question dictionaries\n",
    "\n",
    "    Returns:\n",
    "        List of result dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "    # HINTS:\n",
    "    # 1. Create empty list for results\n",
    "    # 2. For each test question:\n",
    "    #    - Get the question text\n",
    "    #    - Call rag_query() with the question\n",
    "    #    - Add result to results list\n",
    "    # 3. Return results\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Your code here:\n",
    "    for test in test_questions:\n",
    "        # Get question\n",
    "        question = test['question']\n",
    "        # Run RAG query\n",
    "        result = rag_query(question, top_k=TOP_K)\n",
    "\n",
    "        # Attach expected_answer and category to the result\n",
    "        result['expected_answer'] = test.get('expected_answer', '')\n",
    "        result['category'] = test.get('category', 'unknown')\n",
    "\n",
    "        # Store result\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run evaluation on all test questions\n",
    "print(\"Running evaluation on all test questions...\\n\")\n",
    "all_results = run_evaluation(test_questions)\n",
    "\n",
    "print(f\"\\nâœ“ Completed {len(all_results)} tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Display Results\n",
    "\n",
    "Show the evaluation metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics using your functions\n",
    "avg_latency = calculate_average_latency(all_results)\n",
    "successful_retrievals = count_successful_retrievals(all_results)\n",
    "all_sources_used = get_all_sources(all_results)\n",
    "hit_rate = successful_retrievals / len(all_results) if all_results else 0\n",
    "\n",
    "# Display metrics\n",
    "print_separator(\"EVALUATION RESULTS\")\n",
    "print(f\"\\nTotal Questions Tested: {len(all_results)}\")\n",
    "print(f\"Successful Retrievals: {successful_retrievals}\")\n",
    "print(f\"Hit Rate: {hit_rate:.2%}\")\n",
    "print(f\"Average Latency: {avg_latency:.2f} seconds\")\n",
    "print(f\"\\nSources Used: {', '.join(all_sources_used)}\")\n",
    "print_separator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display individual results\n",
    "print(\"\\nIndividual Test Results:\\n\")\n",
    "\n",
    "for i, result in enumerate(all_results, 1):\n",
    "    print(f\"[Test {i}]\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Answer: {result['answer'][:200]}...\")\n",
    "    print(f\"Sources: {', '.join(set(result['sources']))}\")\n",
    "    print(f\"Time: {result['time']:.2f}s\")\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save Your Results\n",
    "\n",
    "Save your test results to a JSON file for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON file\n",
    "results_summary = {\n",
    "    'metrics': {\n",
    "        'total_questions': len(all_results),\n",
    "        'successful_retrievals': successful_retrievals,\n",
    "        'hit_rate': hit_rate,\n",
    "        'average_latency': avg_latency\n",
    "    },\n",
    "    'results': all_results\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Results saved to 'evaluation_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've successfully built a RAG system!\n",
    "\n",
    "### What You Accomplished:\n",
    "âœ… Loaded documents from files  \n",
    "âœ… Chunked text with overlap  \n",
    "âœ… Created a RAG query pipeline  \n",
    "âœ… Built a test dataset  \n",
    "âœ… Calculated evaluation metrics  \n",
    "âœ… Generated a results report  \n",
    "\n",
    "### Next Steps:\n",
    "- Try different chunk sizes and overlaps\n",
    "- Add more test questions\n",
    "- Experiment with different values for `top_k`\n",
    "- Analyze which questions work best\n",
    "- Write up your findings in a report\n",
    "\n",
    "### For Your Report:\n",
    "1. Describe your document collection\n",
    "2. Explain your chunking strategy\n",
    "3. Present your evaluation metrics\n",
    "4. Show examples of good and bad answers\n",
    "5. Discuss what you learned\n",
    "\n",
    "Great job! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
